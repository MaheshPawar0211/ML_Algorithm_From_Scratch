{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis \n",
    "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_len</th>\n",
       "      <th>sepal_wid</th>\n",
       "      <th>petal_len</th>\n",
       "      <th>petal_wid</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_len  sepal_wid  petal_len  petal_wid        class\n",
       "0        5.1        3.5        1.4        0.2  Iris-setosa\n",
       "1        4.9        3.0        1.4        0.2  Iris-setosa\n",
       "2        4.7        3.2        1.3        0.2  Iris-setosa\n",
       "3        4.6        3.1        1.5        0.2  Iris-setosa\n",
       "4        5.0        3.6        1.4        0.2  Iris-setosa"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\n",
    "    filepath_or_buffer='https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "    header=None, \n",
    "    sep=',')\n",
    "\n",
    "df.columns=['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid', 'class']\n",
    "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
    "# split data table into data X and class labels y\n",
    "\n",
    "x = df.drop(columns=\"class\") \n",
    "y = df.iloc[:,4].values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis.\n",
    "More specifically, the reason why it is critical to perform standardization prior to PCA, is that the latter is quite sensitive regarding the variances of the initial variables. That is, if there are large differences between the ranges of initial variables, those variables with larger ranges will dominate over those with small ranges (For example, a variable that ranges between 0 and 100 will dominate over a variable that ranges between 0 and 1), which will lead to biased results. So, transforming the data to comparable scales can prevent this problem.\n",
    "Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std=x.apply(lambda x: (x - np.mean(x)) / (np.std(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Covariance Matrix computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix. \n",
    "What do the covariances that we have as entries of the matrix tell us about the correlations between the variables?\n",
    "\n",
    "It’s actually the sign of the covariance that matters :\n",
    "if positive then : the two variables increase or decrease together (correlated)\n",
    "if negative then : One increases when the other decreases (Inversely correlated)\n",
    "Now, that we know that the covariance matrix is not more than a table that summaries the correlations between all the possible pairs of variables, let’s move to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix \n",
      "           sepal_len  sepal_wid  petal_len  petal_wid\n",
      "sepal_len   1.006711  -0.110103   0.877605   0.823443\n",
      "sepal_wid  -0.110103   1.006711  -0.423338  -0.358937\n",
      "petal_len   0.877605  -0.423338   1.006711   0.969219\n",
      "petal_wid   0.823443  -0.358937   0.969219   1.006711\n"
     ]
    }
   ],
   "source": [
    "cov_mat = (x_std - np.mean(x_std)).T.dot((x_std - np.mean(x_std))) / (x_std.shape[0]-1)\n",
    "print('Covariance matrix \\n%s' %cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is eigenvectors and eigenvalues ref link (https://www.youtube.com/watch?v=IdsV0RaC9jM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the \"core\" of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors \n",
      "[[ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
      " [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
      " [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
      " [ 0.56561105 -0.06541577  0.6338014   0.52354627]]\n",
      "\n",
      "Eigenvalues \n",
      "[2.93035378 0.92740362 0.14834223 0.02074601]\n"
     ]
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Selecting principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes. However, the eigenvectors only define the directions of the new axis, since they have all the same unit length 1, :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Vector :- [ 0.52237162 -0.37231836 -0.72101681  0.26199559]\n",
      "Eigen Vector length :- 0.9999999999999991\n",
      "Eigen Vector :- [-0.26335492 -0.92555649  0.24203288 -0.12413481]\n",
      "Eigen Vector length :- 1.0000000000000004\n",
      "Eigen Vector :- [ 0.58125401 -0.02109478  0.14089226 -0.80115427]\n",
      "Eigen Vector length :- 1.0\n",
      "Eigen Vector :- [ 0.56561105 -0.06541577  0.6338014   0.52354627]\n",
      "Eigen Vector length :- 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "for ev in eig_vecs:\n",
    "    print(\"Eigen Vector :-\",ev)\n",
    "    print(\"Eigen Vector length :-\", np.linalg.norm(ev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide which eigenvector(s) can be dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top k eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues in descending order:\n",
      "2.9303537755893156\n",
      "Corresponding eigen vector [ 0.52237162 -0.26335492  0.58125401  0.56561105]\n",
      "0.9274036215173409\n",
      "Corresponding eigen vector [-0.37231836 -0.92555649 -0.02109478 -0.06541577]\n",
      "0.1483422264816399\n",
      "Corresponding eigen vector [-0.72101681  0.24203288  0.14089226  0.6338014 ]\n",
      "0.020746013995595954\n",
      "Corresponding eigen vector [ 0.26199559 -0.12413481 -0.80115427  0.52354627]\n"
     ]
    }
   ],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])\n",
    "    print(\"Corresponding eigen vector\",i[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many principal components are we going to choose for our new feature subspace is selcted by explained variance\n",
    "The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of variance explained by eig value components:-  72.77045209380135 %\n",
      "Amount of variance explained by eig value components:-  23.030523267680636 %\n",
      "Amount of variance explained by eig value components:-  3.6838319576273912 %\n",
      "Amount of variance explained by eig value components:-  0.515192680890633 %\n",
      "[ 72.77045209  95.80097536  99.48480732 100.        ]\n"
     ]
    }
   ],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "for i in var_exp:\n",
    "    print(\"Amount of variance explained by eig value components:- \",i,\"%\")\n",
    "cum_var_exp = np.cumsum(var_exp)  \n",
    "print(cum_var_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the variance (72.77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain 95.8% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "projection matrix :-  matrix of our concatenated top k eigenvectors.\n",
    "\n",
    "Here, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the \"top 2\" eigenvectors with the highest eigenvalues to construct our d×k-dimensional eigenvector matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix W:\n",
      " [[ 0.52237162 -0.37231836]\n",
      " [-0.26335492 -0.92555649]\n",
      " [ 0.58125401 -0.02109478]\n",
      " [ 0.56561105 -0.06541577]]\n"
     ]
    }
   ],
   "source": [
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "print('Matrix W:\\n', matrix_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = x_std.dot(matrix_w)\n",
    "#print(Y)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# a scatter plot comparing num_children and num_pets\n",
    "Y.plot(kind='scatter',x=0,y=1,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing PCA using python inbuilt methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca.components :- \n",
      " [[ 0.52237162 -0.26335492  0.58125401  0.56561105]\n",
      " [ 0.37231836  0.92555649  0.02109478  0.06541577]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets, sklearn.decomposition\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca.fit(x_std)\n",
    "nComp = 2 # Number of PCA components to consider\n",
    "print(\"pca.components :- \\n\",pca.components_[:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA Components are same as matrix_w we obtain from calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(x_std, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:-\n",
    "1) https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2\n",
    "2) https://plot.ly/ipython-notebooks/principal-component-analysis/\n",
    "3) https://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "4) https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
